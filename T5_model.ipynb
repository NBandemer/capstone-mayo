{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NBandemer/capstone-mayo/blob/t5/T5_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary libraries\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import T5ForSequenceClassification, T5Tokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tensorboard.backend.event_processing import event_accumulator"
      ],
      "metadata": {
        "id": "-DSs6x7k5P13"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PoaQyof32svo"
      },
      "outputs": [],
      "source": [
        "# data preprocessing and train-test split\n",
        "# load the preprocessed dataset from a CSV file\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone/PREPROCESSED-NOTES.csv\")\n",
        "\n",
        "# extract text data and specific SDoH categories from the dataset\n",
        "text_data = dataset[\"text\"].to_list()\n",
        "sdoh_data = {\n",
        "    \"sdoh_community_present\": dataset[\"sdoh_community_present\"].to_list(),\n",
        "    \"sdoh_community_absent\": dataset[\"sdoh_community_absent\"].to_list(),\n",
        "    \"sdoh_education\": dataset[\"sdoh_education\"].to_list(),\n",
        "    \"sdoh_economics\": dataset[\"sdoh_economics\"].to_list(),\n",
        "    \"sdoh_environment\": dataset[\"sdoh_environment\"].to_list(),\n",
        "    \"behavior_alcohol\": dataset[\"behavior_alcohol\"].to_list(),\n",
        "    \"behavior_tobacco\": dataset[\"behavior_tobacco\"].to_list(),\n",
        "    \"behavior_drug\": dataset[\"behavior_drug\"].to_list()\n",
        "}\n",
        "\n",
        "# prepare directories for storing train-test split data for each SDoH category\n",
        "base_path = 'test_train_split/behavior_drug'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "# Iterate through each SDOH data category to split and save as separate CSV files\n",
        "for category, data in sdoh_data.items():\n",
        "    base_path = f\"test_train_split/{category}\"\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Split data for the current category into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        text_data, data, random_state=0, train_size=0.8, stratify=data\n",
        "    )\n",
        "\n",
        "    # Save all splits as CSV files\n",
        "    pd.DataFrame({\"text\": X_train}).to_csv(f\"{base_path}/X_train.csv\", index=False)\n",
        "    pd.DataFrame({\"text\": X_val}).to_csv(f\"{base_path}/X_val.csv\", index=False)\n",
        "    pd.DataFrame({category: y_train}).to_csv(f\"{base_path}/y_train.csv\", index=False)\n",
        "    pd.DataFrame({category: y_val}).to_csv(f\"{base_path}/y_val.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "grqOIcHt24wl"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "\n",
        "def save_metrics_to_csv(json_filepath, csv_filename):\n",
        "    with open(json_filepath) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        log_history = data['log_history']\n",
        "        df = pd.DataFrame(log_history)\n",
        "\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "\n",
        "def plot_metric_from_tensor(log_dir, output_dir, steps_per_epoch):\n",
        "\n",
        "    # Calculate steps_per_epoch based on training data and training arguments\n",
        "    # steps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\n",
        "    event_acc = event_accumulator.EventAccumulator(log_dir)\n",
        "    event_acc.Reload()\n",
        "\n",
        "    graph1_data = event_acc.Scalars(\"eval/loss\")\n",
        "    graph2_data = event_acc.Scalars(\"train/loss\")\n",
        "\n",
        "    # convert steps to epochs\n",
        "    epochs1 = [event.step / steps_per_epoch for event in graph1_data]\n",
        "    values1 = [event.value for event in graph1_data]\n",
        "\n",
        "    epochs2 = [event.step / steps_per_epoch for event in graph2_data]\n",
        "    values2 = [event.value for event in graph2_data]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(epochs1, values1, label=\"Validation Loss\")\n",
        "    plt.plot(epochs2, values2, label=\"Train Loss\")\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Overlap\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save the graph to the specified folder\n",
        "    plt.savefig(os.path.join(output_dir, 'metrics_plot.png'))\n",
        "    plt.show()\n",
        "\n",
        "# evaluation metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions[0].argmax(-1)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def get_latest_checkpoint(folder_path):\n",
        "    # Get a list of all files and directories in the specified folder\n",
        "    files_and_dirs = os.listdir(folder_path)\n",
        "\n",
        "    # Filter only directories (assumed to be checkpoints)\n",
        "    checkpoint_dirs = [d for d in files_and_dirs if os.path.isdir(os.path.join(folder_path, d))]\n",
        "\n",
        "    if not checkpoint_dirs:\n",
        "        print(\"No checkpoint directories found.\")\n",
        "        return None\n",
        "\n",
        "    # Extract the checkpoint numbers from the directory names\n",
        "    checkpoint_numbers = [int(d.split('-')[1]) for d in checkpoint_dirs]\n",
        "\n",
        "    # Identify the directory with the highest checkpoint number\n",
        "    latest_checkpoint = os.path.join(folder_path, f\"checkpoint-{max(checkpoint_numbers)}\")\n",
        "\n",
        "    return latest_checkpoint\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Capstone/checkpoints/checkpoint_epoch_{epoch}.pth'\n",
        "best_model_path = '/content/drive/MyDrive/Colab Notebooks/Capstone/checkpoints/best_model.pth'\n",
        "\n",
        "\n",
        "# Saving the checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, loss, val_loss, is_best=False):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'val_loss': val_loss\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kgr3zQq_u2lB"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "csqlu1lfu2n-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9741c6d0-b82a-45db-a329-0cca27b9839d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForSequenceClassification(\n",
              "  (transformer): T5Model(\n",
              "    (shared): Embedding(32128, 512)\n",
              "    (encoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 512)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 8)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseActDense(\n",
              "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): ReLU()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-5): 5 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseActDense(\n",
              "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): ReLU()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (decoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 512)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 8)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseActDense(\n",
              "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): ReLU()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-5): 5 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseActDense(\n",
              "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): ReLU()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (classification_head): T5ClassificationHead(\n",
              "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (out_proj): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Define label_columns here\n",
        "\"\"\"label_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent', 'sdoh_education',\n",
        "    'sdoh_economics', 'sdoh_environment', 'behavior_alcohol',\n",
        "    'behavior_tobacco', 'behavior_drug'\n",
        "]\"\"\"\n",
        "label_columns = ['sdoh_economics']\n",
        "num_labels = len(label_columns)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", num_labels=num_labels)\n",
        "#tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", num_labels=num_labels)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "configuration = T5ForSequenceClassification.config_class.from_pretrained(\"t5-small\", num_labels=num_labels)\n",
        "#configuration = T5ForSequenceClassification.config_class.from_pretrained(\"t5-small\", num_labels= num_labels)\n",
        "configuration.pad_token_id = tokenizer.pad_token_id\n",
        "model = T5ForSequenceClassification(configuration)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N9CVvlmjwzPb"
      },
      "outputs": [],
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gz_TbUC_3p0y"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone/PREPROCESSED-NOTES.csv\")\n",
        "text_data = dataset[\"text\"].to_list()\n",
        "sdoh_data = dataset[\"sdoh_economics\"].to_list()\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(text_data, sdoh_data, random_state=0, train_size=0.8,\n",
        "                                                  stratify=sdoh_data)\n",
        "max_seq_length = 100\n",
        "\n",
        "# Calculate the number of trainable parameters in the model\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "model_size_MB = num_trainable_params * 4 / (1024 ** 2)\n",
        "effective_batch = 8 / (50*4*model_size_MB) #gpu/seqlength * 4 * model size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sPuxaTGu2qf"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone/PREPROCESSED-NOTES.csv\")\n",
        "\n",
        "text_data = dataset[\"text\"].to_list()\n",
        "sdoh_data = dataset[\"behavior_alcohol\"].to_list()\n",
        "\"\"\"sdoh_data = {\n",
        "    \"sdoh_community_present\": dataset[\"sdoh_community_present\"].to_list(),\n",
        "    \"sdoh_community_absent\": dataset[\"sdoh_community_absent\"].to_list(),\n",
        "    \"sdoh_education\": dataset[\"sdoh_education\"].to_list(),\n",
        "    \"sdoh_economics\": dataset[\"sdoh_economics\"].to_list(),\n",
        "    \"sdoh_environment\": dataset[\"sdoh_environment\"].to_list(),\n",
        "    \"behavior_alcohol\": dataset[\"behavior_alcohol\"].to_list(),\n",
        "    \"behavior_tobacco\": dataset[\"behavior_tobacco\"].to_list(),\n",
        "    \"behavior_drug\": dataset[\"behavior_drug\"].to_list()\n",
        "}\n",
        "\"\"\"\n",
        "base_path = 'test_train_split/behavior_drug'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(text_data, sdoh_data, random_state=0, train_size = .8, stratify=sdoh_data)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=0, test_size = .01)\n",
        "for category, data in sdoh_data.items():\n",
        "    base_path = f\"test_train_split/{category}\"\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Split data for the current category\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        text_data, data, random_state=0, train_size=0.8, stratify=data\n",
        "    ) #maybe try a different test size (0.7/0.3)\n",
        "\n",
        "        # Save all splits as CSV files\n",
        "    pd.DataFrame({\"text\": X_train}).to_csv(f\"{base_path}/X_train.csv\", index=False)\n",
        "    pd.DataFrame({\"text\": X_val}).to_csv(f\"{base_path}/X_val.csv\", index=False)\n",
        "    pd.DataFrame({category: y_train}).to_csv(f\"{base_path}/y_train.csv\", index=False)\n",
        "    pd.DataFrame({category: y_val}).to_csv(f\"{base_path}/y_val.csv\", index=False)\n",
        "\n",
        "max_seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OWo_hibZu2tJ"
      },
      "outputs": [],
      "source": [
        "# define training arguments and start training with the Trainer\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JkZier8Iu2vr"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: change label to float for sdoh_economics\n",
        "\n",
        "class DataLoader(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Retrieve tokenized data for the given index\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            # Add the label for the given index to the item dictionary\n",
        "            item['labels'] = torch.tensor(float(self.labels[idx]))\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing index {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6Aewf93yu2yL"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataLoader(train_encodings, y_train)\n",
        "val_dataset = DataLoader(val_encodings,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K13qpoJCu26R"
      },
      "outputs": [],
      "source": [
        "timestamp_fortrain = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "tensor_logs = f'/content/drive/MyDrive/Colab Notebooks/Capstone/logs/tensor_logs/{timestamp_fortrain}' # create seperate logs for tensor/epoch\n",
        "os.makedirs(tensor_logs, exist_ok=True)\n",
        "epoch_logs = f'/content/drive/MyDrive/Colab Notebooks/Capstone/logs/epoch_logs/{timestamp_fortrain}'\n",
        "os.makedirs(epoch_logs, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i8-ZZN5mu286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cc957dc-4b22-48e7-fcaf-a3b80bc9f441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='616' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [616/616 07:00, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.940300</td>\n",
              "      <td>0.754898</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.501600</td>\n",
              "      <td>0.448171</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.371100</td>\n",
              "      <td>0.339140</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.264400</td>\n",
              "      <td>0.281321</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.209400</td>\n",
              "      <td>0.181604</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.174800</td>\n",
              "      <td>0.186544</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.158651</td>\n",
              "      <td>0.611388</td>\n",
              "      <td>0.463942</td>\n",
              "      <td>0.373795</td>\n",
              "      <td>0.611388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transformer.decoder.embed_tokens.weight'].\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/22 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.15865065157413483, 'eval_accuracy': 0.6113879003558719, 'eval_f1': 0.4639418784502597, 'eval_precision': 0.37379516470156154, 'eval_recall': 0.6113879003558719, 'eval_runtime': 4.8001, 'eval_samples_per_second': 292.703, 'eval_steps_per_second': 4.583, 'epoch': 7.0}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"label_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent', 'sdoh_education',\n",
        "    'sdoh_economics', 'sdoh_environment', 'behavior_alcohol',\n",
        "    'behavior_tobacco', 'behavior_drug'\n",
        "] # change this\n",
        "#label_columns = ['sdoh_community_present']\n",
        "num_labels = len(label_columns)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", num_labels=num_labels)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "configuration = T5ForSequenceClassification.config_class.from_pretrained(\"t5-small\", num_labels= num_labels)\n",
        "configuration.pad_token_id = tokenizer.pad_token_id\n",
        "model = T5ForSequenceClassification(configuration)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
        "\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone/PREPROCESSED-NOTES.csv\")\n",
        "text_data = dataset[\"text\"].to_list()\n",
        "sdoh_data = dataset[\"behavior_alcohol\"].to_list() # change this\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(text_data, sdoh_data, random_state=0, train_size=0.8,\n",
        "                                                  stratify=sdoh_data) #make it test = 0.3\n",
        "max_seq_length = 100  # actually 50 but increase to accomadate outliers\n",
        "\n",
        "# Calculate the number of trainable parameters in the model\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "model_size_MB = num_trainable_params * 4 / (1024 ** 2)\n",
        "effective_batch = 8 / (50*4*model_size_MB) #gpu/seqlength * 4 * model size\n",
        "\n",
        "# define training arguments and start training with the Trainer\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "\n",
        "# custom Dataset class for loading training and validation data\n",
        "class DataLoader(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)  # Converting to tensor , maybe use just 'labels'\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "            item['labels'] = self.labels[idx].clone().detach()  # Already a tensor, just clone and detach\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"index error: {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels) # detach from tensor device\n",
        "\n",
        "# Initialize the DataLoader for training and validation sets with the tokenized encodings\n",
        "train_dataset: DataLoader = DataLoader(\n",
        "    train_encodings,  # These should be the output from the tokenizer\n",
        "    y_train  # These should be labels, as a list or tensor\n",
        ")\n",
        "\n",
        "val_dataset = DataLoader(\n",
        "    val_encodings,  # These should be the output from the tokenizer\n",
        "    y_val  # These should be labels, as a list or tensor\n",
        ")\n",
        "\n",
        "timestamp_fortrain = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "tensor_logs = f'./logs/tensor_logs/{timestamp_fortrain}' #create seperate logs for tensor/epoch\n",
        "os.makedirs(tensor_logs, exist_ok=True)\n",
        "epoch_logs = f'./logs/epoch_logs/{timestamp_fortrain}'\n",
        "os.makedirs(epoch_logs, exist_ok=True)\n",
        "\"\"\"\n",
        "# training args - need to adjust\n",
        "training_args = TrainingArguments(\n",
        "      output_dir= epoch_logs,  # change to epoch log directory, convert to a text\n",
        "      logging_strategy='epoch',  # characterize as epoch\n",
        "      num_train_epochs=7, # have high epoch\n",
        "      #per_device_train_batch_size=64,  # cpu constraint,  64 approp\n",
        "      per_device_train_batch_size=64, #reduced batch sie\n",
        "      per_device_eval_batch_size=64,  # gradient accum if batch size of two, 64 approp\n",
        "      save_strategy= 'epoch',\n",
        "      warmup_steps=500,\n",
        "      weight_decay=1e-5,\n",
        "      logging_dir= tensor_logs,  # change to tensor logs\n",
        "      #eval_steps=100,\n",
        "      evaluation_strategy=\"epoch\",\n",
        "      #accumulate gradients over 4 steps\n",
        "      #gradient_accumulation_steps = 4\n",
        "      load_best_model_at_end=True,  # This will load the best model at the end of training\n",
        "      metric_for_best_model=\"eval_loss\",  # Use eval_loss to determine the best model\n",
        "      greater_is_better=False,  # Set to False because a lower loss is better\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "# evaluate the model\n",
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "# Save checkpoint after every epoch\n",
        "#save_checkpoint(model, optimizer, epoch, current_loss, current_val_loss, is_best=False)\n",
        "\n",
        "#readable results\n",
        "steps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\n",
        "latest_checkpoint = get_latest_checkpoint(epoch_logs) # latest checkpoint update to csv\n",
        "json_path = os.path.join(latest_checkpoint, 'trainer_state.json')\n",
        "save_metrics_to_csv(json_path, 'eval_metric.csv') #update metrics\n",
        "plot_metric_from_tensor(tensor_logs, 'graphs', steps_per_epoch)\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/Colab Notebooks/Capstone/t5\"\n",
        "\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(\"Evaluation Results:\", evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "id": "qvHjlczRfoIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "id": "2y3hG9frfp73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUdxBtaG5BV6"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN\n",
        "trainer.train()\n",
        "\n",
        "# Extract loss values\n",
        "train_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "val_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(train_loss, label='train')\n",
        "plt.plot(val_loss, label='val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Extract evaluation loss values from the log history\n",
        "eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "\n",
        "# evaluation loss for all epochs\n",
        "for epoch, loss in enumerate(eval_losses):\n",
        "    print(f\"Epoch {epoch + 1}: Evaluation Loss = {loss}\")\n",
        "\n",
        "# epoch with the minimum evaluation loss\n",
        "best_epoch = eval_losses.index(min(eval_losses)) + 1\n",
        "print(f'Best epoch: {best_epoch}')\n",
        "\n",
        "\"\"\"test_result = trainer.evaluate(val_dataset)\n",
        "# Report precision, recall, and F1 score\n",
        "y_true = val_dataset['labels']\n",
        "y_pred = trainer.predict(val_dataset).label_ids\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(f'Precision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNyYGquaOlP"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN\n",
        "for epoch, loss in enumerate(train_loss):\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pic9VsIfu3BF"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN\n",
        "# Saving & Loading the model<br>\n",
        "save_directory = \"/content/drive/MyDrive/Colab Notebooks/Capstone/t5\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "283glZPjeRK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "50eb2d6f-74e7-4724-e0f0-e6abb41a17fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d287e9e7bd5d>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Call the function to evaluate on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mevaluate_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-d287e9e7bd5d>\u001b[0m in \u001b[0;36mevaluate_on_test_data\u001b[0;34m(model_path, test_data_path, tokenizer_path, label_columns, max_seq_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Tokenize the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Custom Dataset for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2887\u001b[0m                 )\n\u001b[1;32m   2888\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2890\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3078\u001b[0m         )\n\u001b[1;32m   3079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         batch_outputs = self._batch_prepare_for_model(\n\u001b[0m\u001b[1;32m    808\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    885\u001b[0m         )\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0;31m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Evaluation on Test Data\n",
        "\n",
        "# function to evaluate the trained model on test data\n",
        "def evaluate_on_test_data(model_path, test_data_path, tokenizer_path, label_columns, max_seq_length=512):\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
        "    model = T5ForSequenceClassification.from_pretrained(model_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Load the test data\n",
        "    test_dataset = pd.read_csv(test_data_path)\n",
        "    texts = test_dataset[\"text\"].tolist()\n",
        "\n",
        "    # collect true labels and predictions for each label\n",
        "    true_labels = {label: test_dataset[label].tolist() for label in label_columns}\n",
        "\n",
        "    # Tokenize the test data\n",
        "    test_encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "\n",
        "    # Custom Dataset for test data\n",
        "    class TestDataset(Dataset):\n",
        "        def __init__(self, encodings):\n",
        "            self.encodings = encodings\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            #return {key: val[idx] for key, val in self.encodings.items()}\n",
        "              return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        def __len__(self):\n",
        "            return len(self.encodings.input_ids)\n",
        "\n",
        "    # prepare DataLoader for test data\n",
        "    test_dataset = TestDataset(test_encodings)\n",
        "    test_loader = DataLoader(test_dataset, val_dataset)\n",
        "\n",
        "    # Prepare to collect predictions\n",
        "    predictions = {label: [] for label in label_columns}\n",
        "    results = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        sigmoid_logits = torch.sigmoid(logits).cpu().numpy()  # Apply sigmoid function for binary classification on logits\n",
        "        threshold = 0.5\n",
        "        binary_predictions = (sigmoid_logits > threshold).astype(int)  # Apply threshold to get binary predictions\n",
        "        # Collect binary_predictions for the labels\n",
        "        for i, label in enumerate(label_columns):\n",
        "            predictions[label].extend(binary_predictions[:, i])\n",
        "\n",
        "    # calculate and print the metrics for each label\n",
        "    for label in label_columns:\n",
        "        accuracy = accuracy_score(true_labels[label], predictions[label])\n",
        "        precision = precision_score(true_labels[label], predictions[label], average='weighted')\n",
        "        recall = recall_score(true_labels[label], predictions[label], average='weighted')\n",
        "        f1 = f1_score(true_labels[label], predictions[label], average='weighted')\n",
        "        report = classification_report(true_labels[label], predictions[label], zero_division=0)\n",
        "\n",
        "          # Append the results for the current label to the list\n",
        "        results.append({\n",
        "          'Label': label,\n",
        "          'Accuracy': accuracy,\n",
        "          'Precision': precision,\n",
        "          'Recall': recall,\n",
        "          'F1 Score': f1\n",
        "        })\n",
        "\n",
        "        # Print the results for the current label\n",
        "        print(\"Evaluation results saved to /content/model_evaluation_results.csv\")\n",
        "        print(f\"Metrics for {label}:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Convert the list of results to a DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        results_df.to_csv('/content/model_evaluation_results.csv', index=False)\n",
        "        print(\"Evaluation results saved to /content/model_evaluation_results.csv\")\n",
        "        print(\"Test evaluation completed for all labels.\")\n",
        "\n",
        "# Paths to the model, tokenizer, and test data\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/Capstone/t5\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/Colab Notebooks/Capstone/t5\"\n",
        "test_data_path = \"/content/drive/MyDrive/Colab Notebooks/Capstone/PREPROCESSED-NOTES.csv\"\n",
        "\n",
        "# List of label columns in your test data for evaluation\n",
        "\"\"\"label_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent', 'sdoh_education',\n",
        "    'sdoh_economics', 'sdoh_environment', 'behavior_alcohol',\n",
        "    'behavior_tobacco', 'behavior_drug'\n",
        "]\"\"\"\n",
        "label_columns = ['sdoh_community_present']\n",
        "\n",
        "# Call the function to evaluate on test data\n",
        "evaluate_on_test_data(model_path, test_data_path, tokenizer_path, label_columns, max_seq_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_encodings = tokenizer(text_data, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "test_dataset = DataLoader(test_encodings, sdoh_data)\n",
        "\n",
        "\"\"\"tokenizer = T5Tokenizer.from_pretrained(/content/logs/epoch_logs)\n",
        "model = T5ForSequenceClassification.from_pretrained(tokenizer)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\"\"\"\n",
        "\n",
        "best_checkpoint_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Capstone/t5\")\n",
        "\n",
        "model = T5ForSequenceClassification.from_pretrained(best_checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "#test_metrics = compute_metrics(test_results)\n",
        "\n",
        "results = trainer.predict(test_dataset)\n",
        "print(\"Prediction:\", results)\n",
        "\n",
        "predicted_labels = results.predictions[0].argmax(-1)\n",
        "true_labels = test_dataset.labels\n",
        "# true_labels = test_dataset[label_columns].tolist() #  labels from the DataLoader\n",
        "target_names_binary = ['0', '1', '2']\n",
        "\n",
        "print(\"Test Results:\", test_results)\n",
        "print(\"Classification Report:\\n\", classification_report(true_labels, predicted_labels, target_names=target_names_binary))"
      ],
      "metadata": {
        "id": "yxR5iKE9QHH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45825758-c3bd-493b-c607-0ad9d530a063"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-14-ea7303937015>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: PredictionOutput(predictions=(array([[0.05597382],\n",
            "       [0.00774109],\n",
            "       [2.0584154 ],\n",
            "       ...,\n",
            "       [1.9237925 ],\n",
            "       [1.8426613 ],\n",
            "       [0.52180785]], dtype=float32), array([[[ 2.4178047e-02,  1.2079075e-01, -1.1589122e+00, ...,\n",
            "         -1.3897887e+00, -6.5350145e-01,  9.6981126e-01],\n",
            "        [-4.1470965e-04, -2.5667682e-02,  1.5172446e-02, ...,\n",
            "         -1.8375640e+00, -8.6514205e-01,  5.7786554e-01],\n",
            "        [-3.0387485e-01, -5.8773685e-01, -8.8856995e-01, ...,\n",
            "         -8.2593662e-01, -6.8624932e-01,  6.9053298e-01],\n",
            "        ...,\n",
            "        [ 5.1135319e-01,  9.9627025e-02,  5.9015876e-01, ...,\n",
            "         -1.1598395e+00, -1.7927660e+00,  8.7829089e-01],\n",
            "        [ 5.0939965e-01,  9.9817894e-02,  5.9539509e-01, ...,\n",
            "         -1.1592124e+00, -1.7900918e+00,  8.7853986e-01],\n",
            "        [ 5.0558025e-01,  9.6420489e-02,  5.9431344e-01, ...,\n",
            "         -1.1581925e+00, -1.7920940e+00,  8.7786925e-01]],\n",
            "\n",
            "       [[ 1.2152647e+00,  1.6621917e-01, -9.6057153e-01, ...,\n",
            "         -1.7037675e+00, -9.0004236e-01,  5.4102004e-01],\n",
            "        [-1.3845916e-01,  4.0157971e-01, -1.1063358e+00, ...,\n",
            "         -1.8317289e+00, -4.7754809e-01,  1.4043195e+00],\n",
            "        [-5.3478324e-01,  1.5743206e-01, -1.3114104e+00, ...,\n",
            "         -1.5504113e+00, -8.6523509e-01,  2.0581422e+00],\n",
            "        ...,\n",
            "        [ 1.4989817e-01, -2.4007209e-01,  1.9687021e-01, ...,\n",
            "         -2.2243950e+00, -1.8279921e+00,  2.0625737e+00],\n",
            "        [ 1.4841710e-01, -2.4152178e-01,  1.9521984e-01, ...,\n",
            "         -2.2231276e+00, -1.8293118e+00,  2.0642989e+00],\n",
            "        [ 1.4998683e-01, -2.3928085e-01,  1.9644849e-01, ...,\n",
            "         -2.2244737e+00, -1.8291059e+00,  2.0641510e+00]],\n",
            "\n",
            "       [[-1.8297343e+00,  5.1825017e-01, -2.5501490e+00, ...,\n",
            "          1.4004445e-01,  5.4032129e-01,  2.9131204e-01],\n",
            "        [-1.9998637e+00,  7.9294240e-01, -2.7448058e+00, ...,\n",
            "          4.9576548e-01,  9.6405739e-01, -9.2742816e-02],\n",
            "        [-9.6978384e-01, -1.2139415e-01, -1.9010519e+00, ...,\n",
            "         -2.1181287e-01,  2.1690935e-01,  2.7735668e-01],\n",
            "        ...,\n",
            "        [-1.9014378e+00,  6.4549720e-01, -2.2095358e+00, ...,\n",
            "          6.3995719e-01,  1.0992554e+00, -2.4491116e-01],\n",
            "        [-1.6063704e+00,  7.5215232e-01, -2.1113229e+00, ...,\n",
            "          6.1099225e-01,  7.5902587e-01,  5.1014441e-01],\n",
            "        [-1.5122207e+00,  5.5753893e-01, -1.6039208e+00, ...,\n",
            "         -1.1200776e-01, -9.8058321e-02,  4.9431485e-01]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.2031255e+00, -3.3063862e-01, -1.7739496e+00, ...,\n",
            "          1.2056036e+00,  8.1004131e-01, -6.8170840e-01],\n",
            "        [-1.0169693e+00, -6.5087515e-01, -1.9542625e+00, ...,\n",
            "          9.5861524e-01,  1.0054327e+00, -2.4925718e-01],\n",
            "        [-1.7234261e+00,  1.4073786e-02, -1.4528762e+00, ...,\n",
            "          6.5480912e-01,  9.4270110e-01, -1.6189516e-01],\n",
            "        ...,\n",
            "        [-8.9160877e-01, -2.2251885e-02, -3.5121104e-01, ...,\n",
            "         -5.1103666e-02,  6.3669287e-02,  2.4522041e-01],\n",
            "        [-8.9169234e-01, -2.1857774e-02, -3.5014531e-01, ...,\n",
            "         -5.1553719e-02,  6.4134277e-02,  2.4657100e-01],\n",
            "        [-8.9057004e-01, -2.0673126e-02, -3.4850052e-01, ...,\n",
            "         -5.1498067e-02,  6.5034471e-02,  2.4807321e-01]],\n",
            "\n",
            "       [[-2.5231373e+00, -1.0748254e+00, -1.3307048e+00, ...,\n",
            "          1.0773925e-01,  1.1062258e+00,  1.0024948e+00],\n",
            "        [-2.0025842e+00, -1.0868751e+00, -1.9063849e+00, ...,\n",
            "         -8.1554785e-02,  1.3973124e-01,  6.6627771e-01],\n",
            "        [-2.1110249e+00,  1.0959882e-01, -1.7553995e+00, ...,\n",
            "         -1.3077044e-01, -2.8103632e-01,  8.6741674e-01],\n",
            "        ...,\n",
            "        [-1.2389386e+00, -5.9085536e-01, -3.3388993e-01, ...,\n",
            "         -4.7028068e-01, -4.5254099e-01,  9.3815267e-01],\n",
            "        [-1.2390120e+00, -5.9212685e-01, -3.3463600e-01, ...,\n",
            "         -4.7088155e-01, -4.5208716e-01,  9.3862092e-01],\n",
            "        [-1.2399199e+00, -5.9136814e-01, -3.3446398e-01, ...,\n",
            "         -4.7312903e-01, -4.5101097e-01,  9.3891543e-01]],\n",
            "\n",
            "       [[-3.1636757e-01, -7.6696962e-01, -1.7462113e+00, ...,\n",
            "         -1.5188775e+00, -1.0412608e+00,  1.0821613e+00],\n",
            "        [ 1.4234848e-01, -1.0499239e+00, -2.5728030e+00, ...,\n",
            "         -1.1655624e+00,  6.9019608e-02,  1.4114090e+00],\n",
            "        [ 1.2686990e+00, -2.9091734e-01, -1.5807904e+00, ...,\n",
            "         -1.2010149e+00, -5.5513942e-01,  4.5761290e-01],\n",
            "        ...,\n",
            "        [ 4.6700191e-02, -6.8429703e-01, -1.2847502e+00, ...,\n",
            "         -5.8057505e-01, -3.8639322e-01,  2.0331106e+00],\n",
            "        [-3.5633054e-01, -5.6094930e-02, -1.9620510e+00, ...,\n",
            "         -2.0288057e+00, -5.3581226e-01,  1.8915361e+00],\n",
            "        [ 6.6596186e-01, -1.1272688e+00,  3.9473897e-01, ...,\n",
            "         -1.1245971e+00, -8.1181306e-01,  2.0815136e+00]]], dtype=float32)), label_ids=array([0., 0., 2., ..., 2., 1., 2.], dtype=float32), metrics={'test_loss': 0.11254526674747467, 'test_accuracy': 0.6113879003558719, 'test_f1': 0.46394187845025975, 'test_precision': 0.37379516470156154, 'test_recall': 0.6113879003558719, 'test_runtime': 23.2024, 'test_samples_per_second': 302.771, 'test_steps_per_second': 4.741})\n",
            "Test Results: {'eval_loss': 0.11254526674747467, 'eval_accuracy': 0.6113879003558719, 'eval_f1': 0.46394187845025975, 'eval_precision': 0.37379516470156154, 'eval_recall': 0.6113879003558719, 'eval_runtime': 22.6275, 'eval_samples_per_second': 310.463, 'eval_steps_per_second': 4.861, 'epoch': 7.0}\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      1.00      0.76      4295\n",
            "           1       0.00      0.00      0.00       988\n",
            "           2       0.00      0.00      0.00      1742\n",
            "\n",
            "    accuracy                           0.61      7025\n",
            "   macro avg       0.20      0.33      0.25      7025\n",
            "weighted avg       0.37      0.61      0.46      7025\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Accuracy': [accuracy],\n",
        "    'Precision': [precision],\n",
        "    'Recall': [recall],\n",
        "    'F1 Score': [f1],\n",
        "})\n",
        "print(\"Metrics Table:\\n\", metrics_df)"
      ],
      "metadata": {
        "id": "ffqg7AwNCu8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844cc672-ae67-4bfe-dcb9-1eb63b051f2b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Table:\n",
            "    Accuracy  Precision    Recall  F1 Score\n",
            "0  0.611388   0.373795  0.611388  0.463942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"predicted_labels = results.predictions[0].argmax(axis=1)\n",
        "true_labels = np.array(test_dataset.labels).flatten()\n",
        "\n",
        "true_labels = np.array(test_dataset.labels).flatten()\n",
        "true_labels = np.array(test_dataset.labels).flatten()\"\"\"\n",
        "\n",
        "print(\"True Labels:\", true_labels)\n",
        "print(\"Predicted Labels:\", predicted_labels)\n",
        "\n",
        "print(\"Unique Predicted Labels:\", np.unique(predicted_labels))\n",
        "print(\"Test Dataset Labels:\", np.unique(test_dataset.labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCFTWfcD1Tfb",
        "outputId": "64b1ee39-ddeb-45d2-fd1b-bd5d09b283ad"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Labels: [0 0 2 ... 2 1 2]\n",
            "Predicted Labels: [0 0 0 ... 0 0 0]\n",
            "Unique Predicted Labels: [0]\n",
            "Test Dataset Labels: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#predicted_probs = torch.nn.functional.softmax(torch.tensor(predicted_labels), dim=-1).numpy()\n",
        "\n",
        "# Calculate AUROC\n",
        "auroc = roc_auc_score(true_labels, predicted_labels, multi_class='ovr')\n",
        "print(\"AUROC:\", auroc)"
      ],
      "metadata": {
        "id": "nbm1CKoj0UKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "2b3bd92b-0c5b-46aa-ea62-df0dd9015bc3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "axis 1 is out of bounds for array of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a4a7550b6551>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculate AUROC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUROC:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         return _multiclass_roc_auc_score(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \"\"\"\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# validation of the input y_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         raise ValueError(\n\u001b[1;32m    640\u001b[0m             \u001b[0;34m\"Target scores need to be probabilities for multiclass \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     48\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "12LfmsNjtM7jz3zkYzTbF2SMs3meeF4Wl",
      "authorship_tag": "ABX9TyPyEIx+STlTNDpALvhzr6xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}