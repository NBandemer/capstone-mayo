{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443967ba-4c13-45fc-9e95-ba24a37030df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Prerequisites:\n",
    "#Python 3.10.*\n",
    "#pip install torch, jupyter, transformers, ipywidgets, pandas, re, medspacy\n",
    "\n",
    "#Step 2: Import packages\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AdamW,BertForSequenceClassification,BertTokenizer\n",
    "import medspacy\n",
    "from medspacy.target_matcher import TargetRule\n",
    "from medspacy.visualization import visualize_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Read Notes and Annotations Data\n",
    "df_notes = pd.read_csv(\"../data/NOTEEVENTS.csv\")\n",
    "df_annotations = pd.read_csv('../data/NIHMS1767978-supplement-MIMIC_SBDH.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Filter out data to only include annotated notes and their SBDH values\n",
    "\n",
    "# Columns will be the annotation table columns + TEXT\n",
    "columns = df_annotations.columns.tolist()\n",
    "columns.append(\"TEXT\")\n",
    "annotated_rows = []\n",
    "\n",
    "#Iterate over annotations of MIMIC-III notes and store concatenation of input and outputs\n",
    "for index, row in df_annotations.iterrows():\n",
    "    #get row ID and note text for each entry\n",
    "    rowId = row[\"row_id\"]\n",
    "    note = df_notes.loc[df_notes[\"ROW_ID\"] == rowId]\n",
    "    note = note.iloc[0] #access the first (only) row A.K.A the discharge detail note itself\n",
    "    note = str(note[\"TEXT\"])\n",
    "\n",
    "    #Combine the annotations data with the note\n",
    "    row_list = row.tolist()\n",
    "    row_list.append(note)\n",
    "\n",
    "    #Add to the list of annotated notes\n",
    "    annotated_rows.append(row_list)\n",
    "\n",
    "#Store the list of rows as a dataframe with the described columns, and save\n",
    "df_annotated_notes = pd.DataFrame(annotated_rows, columns=columns)\n",
    "df_annotated_notes.to_csv('../data/ANNOTATEDNOTES.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n",
      "end nlp, start section\n",
      "end section\n",
      "start nlp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nano/Code/ML/Mayo/src/main.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m note \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m\"\u001b[39m\u001b[39mTEXT\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstart nlp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(note)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mend nlp, start section\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m section \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39msections:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyRuSH/PyRuSHSentencizer.py:52\u001b[0m, in \u001b[0;36mPyRuSHSentencizer.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc):\n\u001b[0;32m---> 52\u001b[0m     tags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict([doc])\n\u001b[1;32m     53\u001b[0m     cset_annotations([doc], tags)\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyRuSH/PyRuSHSentencizer.py:60\u001b[0m, in \u001b[0;36mPyRuSHSentencizer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, docs):\n\u001b[1;32m     57\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline's model to a batch of docs, without\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    modifying them.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     guesses \u001b[39m=\u001b[39m cpredict(docs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrush\u001b[39m.\u001b[39;49msegToSentenceSpans)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m guesses\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyRuSH/StaticSentencizerFun.pyx:18\u001b[0m, in \u001b[0;36mPyRuSH.StaticSentencizerFun.cpredict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyRuSH/StaticSentencizerFun.pyx:28\u001b[0m, in \u001b[0;36mPyRuSH.StaticSentencizerFun.cpredict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyRuSH/RuSH.py:109\u001b[0m, in \u001b[0;36mRuSH.segToSentenceSpans\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    107\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m result \u001b[39m=\u001b[39m {BEGIN: [], END: []}\n\u001b[0;32m--> 109\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfastner\u001b[39m.\u001b[39;49mprocess(text, \u001b[39m0\u001b[39;49m, result)\n\u001b[1;32m    111\u001b[0m \u001b[39m# log important message for debugging use\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39misEnabledFor(logging\u001b[39m.\u001b[39mDEBUG):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyFastNER/FastCNER.py:233\u001b[0m, in \u001b[0;36mFastCNER.process\u001b[0;34m(self, text, offset, matches)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(text)):\n\u001b[1;32m    232\u001b[0m     pre_char \u001b[39m=\u001b[39m text[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 233\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessRules(text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrule_map, i, \u001b[39m0\u001b[39;49m, i, matches, pre_char, \u001b[39mFalse\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremovePseudo(matches)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m matches\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyFastNER/FastCNER.py:247\u001b[0m, in \u001b[0;36mFastCNER.processRules\u001b[0;34m(self, text, rule_map, match_begin, match_end, current_position, matches, pre_char, wildcard_support, pre_key)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessWildCards(text, rule_map[\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m], match_begin, match_end, current_position, matches, pre_char,\n\u001b[1;32m    245\u001b[0m                           \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rule_map \u001b[39mand\u001b[39;00m pre_key \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessRules(text, rule_map[\u001b[39m'\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m'\u001b[39;49m], current_position, match_end, current_position, matches, pre_char,\n\u001b[1;32m    248\u001b[0m                       \u001b[39mFalse\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rule_map \u001b[39mand\u001b[39;00m pre_key \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessRules(text, rule_map[\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m], match_begin, current_position, current_position, matches,\n\u001b[1;32m    251\u001b[0m                       pre_char, \u001b[39mFalse\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyFastNER/FastCNER.py:244\u001b[0m, in \u001b[0;36mFastCNER.processRules\u001b[0;34m(self, text, rule_map, match_begin, match_end, current_position, matches, pre_char, wildcard_support, pre_key)\u001b[0m\n\u001b[1;32m    241\u001b[0m this_char \u001b[39m=\u001b[39m text[current_position]\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rule_map:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessWildCards(text, rule_map[\u001b[39m'\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m'\u001b[39;49m], match_begin, match_end, current_position, matches, pre_char,\n\u001b[1;32m    245\u001b[0m                           \u001b[39mTrue\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rule_map \u001b[39mand\u001b[39;00m pre_key \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessRules(text, rule_map[\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m], current_position, match_end, current_position, matches, pre_char,\n\u001b[1;32m    248\u001b[0m                       \u001b[39mFalse\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyFastNER/FastCNER.py:308\u001b[0m, in \u001b[0;36mFastCNER.processWildCards\u001b[0;34m(self, text, rule_map, match_begin, match_end, current_position, matches, pre_char, wildcard_support, pre_key)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mfor\u001b[39;00m rule_char \u001b[39min\u001b[39;00m rule_map\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    307\u001b[0m     \u001b[39mif\u001b[39;00m rule_char \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwildcard_funcs:\n\u001b[0;32m--> 308\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwildcard_funcs[rule_char](text, rule_map, match_begin, match_end, current_position, matches,\n\u001b[1;32m    309\u001b[0m                                        this_char)\n\u001b[1;32m    310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m rule_char \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m is not a eligible syntax.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/PyFastNER/WildCardFunctions.py:84\u001b[0m, in \u001b[0;36mWildCardFunctions.processWildCard_c\u001b[0;34m(self, text, rule_map, match_begin, match_end, current_position, matches, this_char)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessRules(text, rule_map[\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m], match_begin, match_end, current_position \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, matches,\n\u001b[1;32m     81\u001b[0m                           this_char, \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocessWildCard_c\u001b[39m(\u001b[39mself\u001b[39m, text, rule_map, match_begin, match_end, current_position, matches,\n\u001b[1;32m     85\u001b[0m                       this_char):\n\u001b[1;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m this_char\u001b[39m.\u001b[39mislower():\n\u001b[1;32m     87\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessRules(text, rule_map[\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m], match_begin, match_end, current_position \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, matches,\n\u001b[1;32m     88\u001b[0m                           this_char, \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Step 5 Pre-processing\n",
    "\n",
    "#Step 5a Select the desired parts of the clinical note (Social History)\n",
    "# Use the medspacy library\n",
    "nlp = medspacy.load()\n",
    "\n",
    "# Add the sectionizer object to our pipeline, as this is the feature we will use\n",
    "sectionizer = nlp.add_pipe(\"medspacy_sectionizer\")\n",
    "\n",
    "# Validate sectionizer was added to the pipelines\n",
    "nlp.pipe_names\n",
    "\n",
    "# Extract the social history from each note\n",
    "# NOTE: This takes a long time to run\n",
    "note_social_histories = []\n",
    "\n",
    "for index, row in df_annotated_notes.iterrows():\n",
    "    note = str(row[\"TEXT\"])\n",
    "    print('start nlp')\n",
    "    doc = nlp(note)\n",
    "    print('end nlp, start section')\n",
    "    for section in doc._.sections:\n",
    "        if (section.category == 'social_history'):\n",
    "            social_history = section.body_span\n",
    "            social_history_text = doc[social_history[0]:social_history[1]]\n",
    "            note_social_histories.append(social_history_text)\n",
    "            print('end section')\n",
    "            break\n",
    "\n",
    "print(note_social_histories[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "print(len(note_social_histories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to convert output to SBDH presence string\n",
    "#Substance (drug, alcohol, tobacco) classifications\n",
    "sbdh_substance = {\n",
    "    0: 'None',\n",
    "    1: 'Present',\n",
    "    2: 'Past',\n",
    "    3: 'Never',\n",
    "    4: 'Unsure'\n",
    "}\n",
    "\n",
    "#Economics (employed) classifications\n",
    "sbdh_econ_env = {\n",
    "    0: 'None',\n",
    "    1: 'True',\n",
    "    2: 'False',\n",
    "}\n",
    "\n",
    "#Community or Education classifications\n",
    "sbdh_community_ed = {\n",
    "    0: 'False',\n",
    "    1: 'True',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59db360-a6a6-4eae-996d-2aaf4fae4e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4691/2051671803.py:3: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/NOTEEVENTS.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Drop the \"CHARTTIME\" and \"STORETIME\" columns\n",
    "df = df.drop([\"CHARTTIME\", \"STORETIME\", \"CGID\", \"CHARTDATE\"], axis=1)\n",
    "\n",
    "# Drop rows where 'ISERROR' is equal to 1\n",
    "df = df[df['ISERROR'] != 1]\n",
    "\n",
    "# Drop the \"ISERROR\" column\n",
    "df = df.drop([\"ISERROR\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e69c2d6d-4560-4632-8939-27db2c4e794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 0, 0, 0, 0, 0, 1, 0, 'Admission Date:  [**2190-5-16**]     Discharge Date:  [**2190-5-22**]\\n\\nDate of Birth:   [**2139-4-22**]     Sex:  F\\n\\nService:  CARDIOTHORACIC\\n\\nHISTORY OF PRESENT ILLNESS:  This 51 year-old female was\\nadmitted to an outside hospital with chest pain and ruled in\\nfor myocardial infarction.  She was transferred here for a\\ncardiac catheterization.\\n\\nPAST MEDICAL HISTORY:  Hypertension, fibromyalgia,\\nhypothyroidism, NASH and noninsulin dependent diabetes.\\n\\nPAST SURGICAL HISTORY:  Hysterectomy and cholecystectomy.\\n\\nSOCIAL HISTORY:  She smokes a pack per day.\\n\\nMEDICATIONS ON ADMISSION:  Hydrochlorothiazide, Alprazolam,\\nUrsodiol and Levoxyl.\\n\\nShe was hospitalized with Aggrastat, nitroglycerin and\\nheparin as she ruled in for myocardial infarction.\\n\\nALLERGIES:  No known drug allergies.\\n\\nCardiac catheterization showed left anterior descending\\ncoronary artery diagonal 80% lesion, circumflex 90% lesion\\nand 90% lesion of the right coronary artery with a normal\\nejection fraction.  She was transferred from [**Hospital3 68**]\\nto [**Hospital1 69**] for cardiac\\ncatheterization.  The results as above.  After\\ncatheterization she was referred to cardiothoracic surgery\\nand was seen by Dr. [**First Name8 (NamePattern2) **] [**Last Name (NamePattern1) 70**] and Dr. [**First Name4 (NamePattern1) 71**] [**Last Name (NamePattern1) 72**].\\nPreoperative laboratories showed a sodium of 141, K 4.2,\\nchloride 105, CO2 24, BUN 12, creatinine 0.6 with a blood\\nsugar of 156.  White count 8.9, hematocrit 44.2, platelet\\ncount 201,000.  PT 13, PTT 26 with an INR of 1.2.  CK was\\n1511 on [**5-16**].  She was also followed by Dr. [**Last Name (STitle) 73**] of\\ncardiology and agreed to participate in both the Cariporide\\nand Dermabond studies through cardiac surgery.  The patient\\nwas taken to the Operating Room on [**5-18**] and underwent\\ncoronary artery bypass grafting times four with a left\\ninternal mammary coronary artery to the left anterior\\ndescending coronary artery, saphenous vein graft to right\\nposterior descending coronary artery, saphenous vein graft to\\ndiagonal two and a saphenous vein graft to the obtuse\\nmarginal by Dr. [**Last Name (STitle) 70**].\\n\\nThe patient was transferred to the Cardiothoracic Intensive\\nCare Unit in stable condition.  On postoperative day number\\none there were no events overnight. The patient was\\nextubated and was on a neo-synephrine drip at 0.3 micrograms\\nper kilo per minute with the Cariporide infusing.\\nNitroglycerin had been turned off.  Postoperative hematocrit\\nwas 30 with a K of 4.2 and a blood sugar of 139.  CPK trended\\ndown to 357 and 379 with an MB of 15 to 16.  The patient was\\nin sinus rhythm in the 80s with a stable blood pressure.  She\\nwas alert and oriented.  Her lungs were clear bilaterally.\\nHeart was regular rate and rhythm.  Her abdomen was benign.\\nHer extremities were within normal limits.  She was\\nneurologically stable.  Her chest tubes were pulled on\\npostoperative day number three.  She continued on\\nperioperative antibiotics and was transferred out to the\\nfloor.\\n\\nShe was seen by physical therapy for evaluation.  On\\npostoperative day two she had no events overnight.  She had a\\ntemperature max of 100.6.  Her JP drain from her leg site was\\nremoved as was her Foley.  Her Lopresor was increased to 50\\nb.i.d.  She began to ambulate and was out of bed.  She had\\ndecreased at the bases, but was otherwise hemodynamically\\nstable.  Her dressings were clean, dry and intact.  She was\\nseen by case management to determine the need for rehab.  Her\\npacing wires were discontinued on postoperative day three.\\nShe continued to advance her ambulation.  She had decreased\\nbreath sounds a the bases again on postoperative day three,\\nbut was stable and continuing to increase her physical\\ntherapy.  Her incision was were clean, dry and intact.  Pain\\nwas managed with Percocet and Motrin.  She was sating 92% on\\nroom air on postoperative day number four the day of\\ndischarge with a temperature max of 99.3, blood pressure\\n136/71, heart rate 93.  She was alert, oriented and had been\\nambulating well.  Her lungs were clear bilaterally.  Her\\nexamination was otherwise benign.\\n\\nHer laboratories on the 9th showed a white count of 13.6,\\nhematocrit 28.7, platelet count 153,000, BUN 15, creatinine\\n0.5, sodium 141, glucose 100, K 3.8, magnesium 1.7 for which\\nshe received 2 grams of repletion.  Calcium 1.08 for which\\nshe received 2 grams of repletion.  She was discharged to\\nhome on postoperative day four [**5-22**].\\n\\nDISCHARGE MEDICATIONS:  Lasix 20 mg po q.d. times one week,\\nK-Ciel 20 milliequivalents po q day times one week.  Colace\\n100 mg po q.d., Zantac 150 mg po b.i.d., enteric coated\\naspirin 325 mg po q day, Levoxyl 0.25 mg po q day, Lopressor\\n75 mg po b.i.d., Nicoderm 14 patch q.d., Xanax 2 mg q 4 to 6\\nhours prn, Ursodiol dosage not specified.  The patient was\\ninstructed to return to preoperative dose.  Percocet one to\\ntwo tabs po prn q 4 to 6 hours.\\n\\nThe patient was afebrile.  Incisions were healing well.\\n\\nDISCHARGE DIAGNOSES:\\n1.  Hypertension.\\n2.  Status post coronary artery bypass grafting times four.\\n3.  Fibromyalgia.\\n4.  Hypothyroidism.\\n5.  Noninsulin dependent diabetes mellitus.\\n6.  Question NASH.\\n\\nShe was also instructed to follow up with her primary care\\nphysician [**Last Name (NamePattern4) **]. [**Last Name (STitle) 74**] in two weeks and follow up with Dr.\\n[**Last Name (STitle) 70**] in the office in six weeks for postop follow up.\\nAgain, the patient was discharged home on [**2190-5-22**].\\n\\n\\n\\n\\n\\n\\n\\n\\n                          [**First Name11 (Name Pattern1) **] [**Initials (NamePattern4) **] [**Last Name (NamePattern4) **], M.D.  [**MD Number(1) 75**]\\n\\nDictated By:[**Last Name (NamePattern1) 76**]\\n\\nMEDQUIST36\\n\\nD:  [**2190-7-7**]  08:16\\nT:  [**2190-7-7**]  11:56\\nJOB#:  [**Job Number 77**]\\n']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af71fb8-39c5-4e55-be6a-0e58e2fecb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noteText = str(note[\"TEXT\"])\n",
    "category = str(note[\"CATEGORY\"])\n",
    "\n",
    "if category.lower() != \"discharge summary\":\n",
    "    print(category)\n",
    "\n",
    "#get start, end indices and sbdh name from keywords table\n",
    "keywords_data = keywords.loc[keywords[\"row_id\"] == rowId]\n",
    "keyword_entries = keywords_data[[\"start\",\"end\",\"sbdh\"]]\n",
    "\n",
    "#Iterate over each keyword entry\n",
    "for sindex, srow in keyword_entries.iterrows():\n",
    "    #Get the sbdh referenced, as well as its classified value, and indices of keyword\n",
    "    sbdh = srow[\"sbdh\"]\n",
    "\n",
    "    if \"community\" in sbdh:\n",
    "        sbdh = \"sdoh_community_present\" #the keywords table does not specify absence or presence, so assume presence\n",
    "    \n",
    "    sbdh_value = row[sbdh]\n",
    "    \n",
    "    sbdh_term = \"\"\n",
    "    start, end = srow[\"start\"], srow[\"end\"]\n",
    "    \n",
    "    #Map the classified value to corresponding string (Present, Past, etc.)\n",
    "    if \"behavior\" in sbdh:\n",
    "        sbdh_term = sbdh_substance[sbdh_value]\n",
    "    elif \"community\" in sbdh or \"education\" in sbdh:\n",
    "        sbdh_term = sbdh_community_ed[sbdh_value]\n",
    "    else:\n",
    "        sbdh_term = sbdh_econ_env[sbdh_value]\n",
    "\n",
    "    #Print each sbdh found, and the keyword\n",
    "    #print(rowId, sbdh, sbdh_term, noteText[start:end])\n",
    "\n",
    "    #Print context surrounding keyword\n",
    "    print('Context: ', noteText[start-20:end+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (2.1.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: typing-extensions in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: filelock in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.52)\n",
      "Requirement already satisfied: requests in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: numpy in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading https://download.pytorch.org/whl/Pillow-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nano/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: pillow, torchvision, torchaudio\n",
      "Successfully installed pillow-9.3.0 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# pip install transformers\n",
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/nano/Code/ML/Mayo/src/main.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertForSequenceClassification, BertTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Put the model into training mode\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nano/Code/ML/Mayo/src/main.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mtrain() \n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1250\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1251\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/import_utils.py:1230\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[39m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1232\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "#Initialize tokenizer and model from pretrained Bio_ClinicalBert\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "#Put the model into training mode\n",
    "model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer \n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
