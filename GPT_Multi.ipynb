{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code for multiple models."
      ],
      "metadata": {
        "id": "Z8AYKehoOcCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypT-d4FHNxGA",
        "outputId": "00b23e0c-b559-4713-ee96-bcd7d39c32d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: plotting in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from plotting) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from plotting) (0.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from plotting) (1.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->plotting) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->plotting) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# CELL 1\n",
        "# import necessary libraries for data manipulation, model evaluation, and plotting\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW  # variant of Adam with weight decay\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import datetime\n",
        "import warnings\n",
        "import shutil\n",
        "import sklearn\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "!pip install transformers[torch] accelerate -U plotting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2\n",
        "# Data preprocessing and helper functions\n",
        "\n",
        "# Separate models for each SDOH\n",
        "\n",
        "current_sdoh = \"behavior_alcohol\"\n",
        "\n",
        "# community present/absent and education\n",
        "sdoh_community_education = {\n",
        "      0: 'False',\n",
        "      1: 'True',\n",
        "}\n",
        "\n",
        "# economics and environment\n",
        "sdoh_economics_environment = {\n",
        "      0: 'None',\n",
        "      1: 'True',\n",
        "      2: 'False',\n",
        "}\n",
        "\n",
        "# alcohol, drug, tobacco\n",
        "sdbh_alcohol_drug_tobacco = {\n",
        "      0: 'None',\n",
        "      1: 'Present',\n",
        "      2: 'Past',\n",
        "      3: 'Never',\n",
        "      4: 'Unsure'\n",
        "}\n",
        "\n",
        "# function to determine which SDOH is described\n",
        "# here, the SDOH for the classification report will be set\n",
        "def set_sdoh(sdoh_group):\n",
        "  global current_sdoh\n",
        "  current_sdoh = sdoh_group\n",
        "\n",
        "''' balance_data function balances the classes in a dataset by upsamling/oversampling\n",
        "the minority classes to match the number of samples in the majority class '''\n",
        "def balance_data(df):\n",
        "    values = df['y'].value_counts() # calculates the frequency of each class in the 'y' column of the df (identifies majority/minority based on occurrence)\n",
        "    majority = df[df['y'] == values.idxmax()] # identifies majority class, selects class with highest frequency\n",
        "    desired_samples = len(majority) # this number will be used as target number of samples after samples\n",
        "\n",
        "    # iterate through each label in the dataset and check if current label is the majority class\n",
        "    for label in values.index:\n",
        "        if label == values.idxmax():\n",
        "            continue\n",
        "        minority = df[df['y'] == label]\n",
        "        upsampled_minority = resample(minority,\n",
        "                                      replace=True,  # Sample with replacement\n",
        "                                      n_samples=desired_samples,  # Match number of majority class\n",
        "                                      random_state=42)\n",
        "        majority = pd.concat([majority, upsampled_minority])\n",
        "\n",
        "    return majority\n",
        "\n",
        " # new function to calculate class weights for the given data\n",
        "def get_class_weights(y):\n",
        "   class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "   return torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# TEST TRAIN SPLIT\n",
        "# for all SDoH from the preprocessed data\n",
        "dataset = pd.read_csv(\"/content/PREPROCESSED-NOTES.csv\")\n",
        "\n",
        "# prepare directories for storing train-test split data for each SDoH category\n",
        "base_path = '/content/test_train_split'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "sdoh_data = {\n",
        "    \"sdoh_community_present\": dataset[\"sdoh_community_present\"].to_list(),\n",
        "    \"sdoh_community_absent\": dataset[\"sdoh_community_absent\"].to_list(),\n",
        "    \"sdoh_education\": dataset[\"sdoh_education\"].to_list(),\n",
        "    \"sdoh_economics\": dataset[\"sdoh_economics\"].to_list(),\n",
        "    \"sdoh_environment\": dataset[\"sdoh_environment\"].to_list(),\n",
        "    \"behavior_alcohol\": dataset[\"behavior_alcohol\"].to_list(),\n",
        "    \"behavior_tobacco\": dataset[\"behavior_tobacco\"].to_list(),\n",
        "    \"behavior_drug\": dataset[\"behavior_drug\"].to_list()\n",
        "}\n",
        "\n",
        "\n",
        "# extract text data and specific SDoH categories from the dataset\n",
        "text_data = dataset[\"text\"].to_list()\n",
        "\n",
        " # Iterate through each SDOH data category to split and save as separate CSV files\n",
        "for category, data in sdoh_data.items():\n",
        "    category_path = os.path.join(base_path, category)  # Build the full path for the category\n",
        "    os.makedirs(category_path, exist_ok=True)  # Ensure the category directory exists\n",
        "\n",
        "    # Split data for the current category into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        text_data, data, test_size=0.2, random_state=0, stratify=data\n",
        "    )\n",
        "\n",
        "    # Save all splits as CSV files\n",
        "    pd.DataFrame({\"text\": X_train, category: y_train}).to_csv(os.path.join(category_path, 'train.csv'), index=False)\n",
        "    pd.DataFrame({\"text\": X_val, category: y_val}).to_csv(os.path.join(category_path, 'test.csv'), index=False)"
      ],
      "metadata": {
        "id": "wxTaB3aoOkZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3\n",
        "# Relevant information for the training and testing models are in this cell\n",
        "\n",
        "class SDOHDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "  def compute_metrics(self, eval_pred):\n",
        "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
        "    return compute_metrics((eval_pred.label_ids, preds))\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"\n",
        "        This function computes the loss for the given model and inputs\n",
        "        \"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = torch.nn.functional.cross_entropy(logits, labels, weight=self.weights if hasattr(self, 'weights') else None)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n"
      ],
      "metadata": {
        "id": "5Tkj1bmqU2Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4\n",
        "class Model():\n",
        "    def __init__(self, Sdoh_name, num_of_labels, model_name, epochs, batch, project_base_path, balanced, weighted, output_dir=None, cv=None):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer and model for the class to use\n",
        "        \"\"\"\n",
        "        # Suppress FutureWarning messages\n",
        "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "        # Ensure the tokenizer accepts a pad token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        # Initialize the model for sequence classification with the specified number of labels\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_of_labels)\n",
        "\n",
        "         # Add pad token id to model configuration if not already set\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.Sdoh_name = Sdoh_name\n",
        "        self.num_of_labels = num_of_labels\n",
        "        self.epochs = epochs\n",
        "        self.batch = batch\n",
        "        self.project_base_path = project_base_path\n",
        "        self.balanced = balanced\n",
        "        self.weighted = weighted\n",
        "        self.output_dir = output_dir\n",
        "        self.cv = cv\n",
        "\n",
        "    def train(self):\n",
        "        data_path = os.path.join(self.project_base_path, f\"/content/test_train_split/{self.Sdoh_name}/\")\n",
        "        data_file = 'train.csv'\n",
        "        df = pd.read_csv(os.path.join(data_path, data_file))\n",
        "        df.dropna(subset=['text'], inplace=True)\n",
        "        x = df['text']\n",
        "        y = df[self.Sdoh_name]\n",
        "\n",
        "        MAX_LENGTH = 512\n",
        "        early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "        optimizer = AdamW(self.model.parameters(), lr=5e-5)\n",
        "        current_fold = 1\n",
        "\n",
        "        if self.weighted:\n",
        "            self.weights = get_class_weights(y)\n",
        "\n",
        "        if self.cv:\n",
        "            # Implement 5-fold stratified cross val\n",
        "            skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "            split_iterator = skf.split(x, y)\n",
        "\n",
        "        else:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
        "            max_index = len(df) - 1  # Get the maximum valid index of the dataframe\n",
        "            split_iterator = [(list(X_train.index[X_train.index <= max_index]), list(X_val.index[X_val.index <= max_index]))]\n",
        "\n",
        "        for train, test in split_iterator:\n",
        "            X_train = x.iloc[train]\n",
        "            X_val = x.iloc[test]\n",
        "\n",
        "            X_train, X_val = x.iloc[train], x.iloc[test]\n",
        "            y_train, y_val = y.iloc[train], y.iloc[test]\n",
        "            epoch_training_steps = len(X_train) // self.batch\n",
        "            num_training_steps = epoch_training_steps * self.epochs\n",
        "            num_warmup_steps = epoch_training_steps * 0.1\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "            optimizers = (optimizer, scheduler)\n",
        "\n",
        "            # Handle class imbalance in training data\n",
        "            if self.balanced:\n",
        "                df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n",
        "                balanced_train = balance_data(df_train)\n",
        "\n",
        "                # Convert back to lists if needed\n",
        "                list_train_x = balanced_train['X'].tolist()\n",
        "                list_train_y = balanced_train['y'].tolist()\n",
        "            else:\n",
        "                list_train_x = X_train.tolist()\n",
        "                list_train_y = y_train.tolist()\n",
        "\n",
        "            list_val_x = X_val.tolist()\n",
        "            list_val_y = y_val.tolist()\n",
        "\n",
        "            train_encodings = self.tokenizer(list_train_x, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
        "            val_encodings = self.tokenizer(list_val_x, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
        "\n",
        "            train_dataset: SDOHDataset(\n",
        "              train_encodings,  # These should be the output from the tokenizer\n",
        "              list_train_y  # These should be labels, as a list or tensor\n",
        "            )\n",
        "\n",
        "            val_dataset = SDOHDataset(\n",
        "              val_encodings,  # These should be the output from the tokenizer\n",
        "              list_val_y  # These should be labels, as a list or tensor\n",
        "            )\n",
        "\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "            log_dir = self.project_base_path if self.output_dir is None else self.output_dir\n",
        "\n",
        "            tensor_logs = os.path.join(log_dir, f'logs/{self.Sdoh_name}/tensor_logs/logs_{timestamp}')\n",
        "            os.makedirs(tensor_logs, exist_ok=True)\n",
        "\n",
        "            epoch_logs = os.path.join(log_dir, f'logs/{self.Sdoh_name}/epoch_logs/logs_{timestamp}')\n",
        "            os.makedirs(epoch_logs, exist_ok=True)\n",
        "\n",
        "            # Model training code here\n",
        "            # training args - need to adjust\n",
        "            training_args = TrainingArguments(\n",
        "              output_dir= epoch_logs,\n",
        "              logging_strategy='epoch',\n",
        "              num_train_epochs=self.epochs, # 4\n",
        "              per_device_train_batch_size= self.batch, #16\n",
        "              per_device_eval_batch_size= self.batch, #64\n",
        "              save_strategy= 'epoch',\n",
        "              warmup_steps=500,\n",
        "              weight_decay=1e-5,\n",
        "              logging_dir= tensor_logs,\n",
        "              evaluation_strategy=\"epoch\",\n",
        "              load_best_model_at_end=True,\n",
        "              metric_for_best_model=\"eval_loss\",\n",
        "              #greater_is_better=False,   Set to False because a lower loss is better\n",
        "            )\n",
        "\n",
        "            trainer = CustomTrainer(\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=train_dataset,\n",
        "              eval_dataset=val_dataset,\n",
        "              compute_metrics=compute_metrics,\n",
        "              callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # 3 is a balance between giving the model enough chance  to improve and stopping early enough to prevent overfitting and unnecessary computation\n",
        "            )\n",
        "\n",
        "            print(f'Starting Training{\" Fold \" + str(current_fold) if self.cv else \"\"}')\n",
        "\n",
        "            # train the model\n",
        "            trainer.train()\n",
        "\n",
        "            print(f'Finished Training{\" Fold \" + str(current_fold) if self.cv else \"\"}')\n",
        "\n",
        "            graph_dir = os.path.join(self.project_base_path, f'graphs/{self.Sdoh_name}')\n",
        "            save_dir = os.path.join(self.project_base_path, f'saved_models/{self.Sdoh_name}')\n",
        "\n",
        "            # Configure directory paths depending on config\n",
        "            if self.cv:\n",
        "                graph_dir += f'_cv{current_fold}'\n",
        "                save_dir += f'_cv{current_fold}'\n",
        "            if self.balanced:\n",
        "                graph_dir += '_balanced'\n",
        "                save_dir += '_balanced'\n",
        "            if self.weighted:\n",
        "                graph_dir += '_weighted'\n",
        "                save_dir += '_weighted'\n",
        "\n",
        "            # Create directories\n",
        "            os.makedirs(graph_dir, exist_ok=True)\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "            # Plot and save\n",
        "            plot_metric_from_tensor(tensor_logs, f'{graph_dir}/plot_loss.jpg')\n",
        "            self.model.save_pretrained(save_dir)\n",
        "            self.tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "            current_fold += 1\n",
        "\n",
        "    def test(self):\n",
        "        set_helper_sdoh(self.Sdoh_name)\n",
        "\n",
        "        data_path = os.path.join(self.project_base_path, f\"/content/test_train_split/{self.Sdoh_name}\")\n",
        "\n",
        "        data_file = 'test.csv'\n",
        "        test_df = pd.read_csv(os.path.join(data_path, data_file))\n",
        "\n",
        "        test_df.dropna(subset=['text'], inplace=True)\n",
        "        test_inputs = test_df['text'].tolist()\n",
        "        test_labels = test_df[self.Sdoh_name].tolist()\n",
        "\n",
        "        max_seq_length = 128\n",
        "\n",
        "        test_encodings = self.tokenizer(test_inputs, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt')\n",
        "\n",
        "        test_dataset = MIMICDataset(\n",
        "            test_encodings,\n",
        "            test_labels\n",
        "        )\n",
        "\n",
        "        saved_models_dir = os.path.join(self.project_base_path, f'saved_models/')\n",
        "        sdoh_dir = f'{self.Sdoh_name}'\n",
        "\n",
        "        if self.cv:\n",
        "            sdoh_dir += f'_cv5'\n",
        "        if self.balanced:\n",
        "            sdoh_dir += '_balanced'\n",
        "        if self.weighted:\n",
        "            sdoh_dir += '_weighted'\n",
        "\n",
        "        results_dir = os.path.join(self.project_base_path, f'test_results/{sdoh_dir}')\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        model =  AutoModelForSequenceClassification.from_pretrained(os.path.join(saved_models_dir, sdoh_dir))\n",
        "\n",
        "        eval_trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            test=True,\n",
        "            eval_dataset=test_dataset,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        results = eval_trainer.evaluate()\n",
        "\n",
        "        cm = results.get('eval_cm')\n",
        "\n",
        "        cm.plot()\n",
        "        plt.savefig(f\"{results_dir}/confusion_matrix.jpg\")\n",
        "\n",
        "        curves = results.get('eval_roc')\n",
        "        roc_dir = os.path.join(results_dir, 'roc')\n",
        "        os.makedirs(roc_dir, exist_ok=True)\n",
        "\n",
        "        for display, best_threshold in curves:\n",
        "            # Plot the ROC curve\n",
        "            display.plot()\n",
        "\n",
        "            # Set titles and labels\n",
        "            plt.title(f'ROC Curve for {display.estimator_name}')\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "\n",
        "            # Add annotation for the best threshold\n",
        "            plt.text(0.5, 0.5, f'Best Threshold: {best_threshold:.2f}', ha='center', va='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "            # Save the figure as JPG with the estimator name\n",
        "            plt.savefig(f'{roc_dir}/{display.estimator_name}.jpg')\n",
        "\n",
        "        report_df = results.get('eval_classification_report')\n",
        "        report_df.to_csv(f\"{results_dir}/classification_report.csv\")\n",
        "\n",
        "         # Save evaluation results to a CSV file\n",
        "        results_df = pd.DataFrame([results]).drop(columns=['eval_cm', 'eval_roc', 'eval_classification_report'])\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        results_df.to_csv(f\"{results_dir}/results.csv\", index=False)\n",
        "        print(\"Evaluation results saved to:\", results_dir)\n",
        "\n",
        "        # Clean up temp files\n",
        "        tmp_dir = os.path.join(os.getcwd(), 'tmp_trainer')\n",
        "        if os.path.exists(tmp_dir):\n",
        "            shutil.rmtree(tmp_dir)"
      ],
      "metadata": {
        "id": "jr2NPWnEa8oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_base_path = Path(\"/content\").resolve()\n",
        "train_dataset_path = project_base_path /\"/content/PREPROCESSED-NOTES.csv\"  # Adjust as necessary\n",
        "test_dataset_path = project_base_path /\"/content/ANNOTATEDNOTES.csv\""
      ],
      "metadata": {
        "id": "JakmuGmSnNvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5\n",
        "def compute_metrics(eval_pred):\n",
        "    labels = eval_pred.label_ids\n",
        "    logits = eval_pred.predictions\n",
        "    logits_tensor = torch.tensor(logits)  # Convert logits to PyTorch tensor\n",
        "    preds_probs_tensor = F.softmax(logits_tensor, dim=-1)  # Apply softmax along the last dimension\n",
        "    preds_probs = preds_probs_tensor.numpy()  # Convert probabilities back to numpy array\n",
        "\n",
        "    # PREDICTIONS\n",
        "    preds = np.argmax(preds_probs, axis=-1)\n",
        "\n",
        "    # Classifier Metrics\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # classification report\n",
        "    if current_sdoh.startswith(\"behavior\"):\n",
        "        current_sbdh_dict = sdbh_alcohol_drug_tobacco\n",
        "    elif current_sdoh == \"sdoh_economics\" or current_sdoh == \"sdoh_environment\":\n",
        "        current_sbdh_dict = sdoh_economics_environment\n",
        "    else:  # This includes 'sdoh_community_education'\n",
        "        current_sbdh_dict = sdoh_community_education\n",
        "\n",
        "    report = classification_report(labels, preds, target_names=current_sbdh_dict.values(), output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Metrics for predicted probabilities\n",
        "    # AUC score\n",
        "    if num_classes > 2:\n",
        "        auc = roc_auc_score(labels, preds_probs, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        greater_class_prob = preds_probs[:, 1]\n",
        "        auc = roc_auc_score(labels, greater_class_prob, average='weighted', multi_class='ovr')\n",
        "\n",
        "    if test:\n",
        "        # Confusion Matrix\n",
        "        cm = ConfusionMatrixDisplay.from_predictions(labels, preds)\n",
        "\n",
        "        # ROC Curve\n",
        "        # Handle multi class ROC curves using OvR\n",
        "        curves = []\n",
        "        if num_classes > 2:\n",
        "            for i in range(num_classes):\n",
        "                fpr, tpr, thresholds = roc_curve(labels, preds_probs[:, i], pos_label=i)\n",
        "                display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name=f'{current_sbdh}_{current_sbdh_dict[i]}')\n",
        "                best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "                curves.append((display, best_threshold))\n",
        "        else:\n",
        "            fpr, tpr, thresholds = roc_curve(labels, greater_class_prob, pos_label=1)\n",
        "            display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='Estimator')\n",
        "            best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "            curves.append((display, best_threshold))\n",
        "\n",
        "        return {\n",
        "          'accuracy': acc,\n",
        "          'f1': f1,\n",
        "          'precision': precision,\n",
        "          'recall': recall,\n",
        "          'classification_report': report_df,\n",
        "          'roc': curves,\n",
        "          'cm': cm,\n",
        "        }\n",
        "\n",
        "def plot_metric_from_tensor(log_dir, output_dir, steps_per_epoch):\n",
        "\n",
        " # Calculate steps_per_epoch based on training data and training arguments\n",
        "    event_acc = event_accumulator.EventAccumulator(log_dir)\n",
        "    event_acc.Reload()\n",
        "\n",
        "    graph1_data = event_acc.Scalars(\"eval/loss\")\n",
        "    graph2_data = event_acc.Scalars(\"train/loss\")\n",
        "\n",
        "    training_dataset_size = len(X_train)  # Size of your training dataset\n",
        "    total_steps_per_epoch = math.ceil(training_dataset_size / self.batch)\n",
        "\n",
        "    # convert steps to epochs #\n",
        "    epochs1 = [event.step / total_steps_per_epoch for event in graph1_data]\n",
        "    values1 = [event.value for event in graph1_data]\n",
        "\n",
        "    epochs2 = [event.step / total_steps_per_epoch for event in graph2_data]\n",
        "    values2 = [event.value for event in graph2_data]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(epochs1, values1, label=\"Eval Loss\")\n",
        "    plt.plot(epochs2, values2, label=\"Train Loss\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Overlap\")\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save the graph to the specified folder\n",
        "    plt.savefig(os.path.join(output_dir, 'metrics_plot.png'))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AnriYsJil5R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### NEW\n",
        "config = {\n",
        "    'sdoh': 'behavior_alcohol',  # or any other SDOH category you're working with\n",
        "    'model': 'gpt2',  # or any model you're planning to use\n",
        "    'epochs': 4,\n",
        "    'batch': 16,\n",
        "    'balanced': True,\n",
        "    'weighted': True,\n",
        "    'output': 'path_to_output_directory',\n",
        "    'cv': False\n",
        "}\n",
        "\n",
        "sdoh_label_counts = {\n",
        "    'sdoh_community_present': 2,\n",
        "    'sdoh_community_absent': 2,\n",
        "    'sdoh_education': 2,\n",
        "    'sdoh_economics': 3,  # Assuming 'None', 'True', 'False'\n",
        "    'sdoh_environment': 3,  # Assuming 'None', 'True', 'False'\n",
        "    'behavior_alcohol': 5,  # 'None', 'Present', 'Past', 'Never', 'Unsure'\n",
        "    'behavior_tobacco': 5,  # Similar to alcohol\n",
        "    'behavior_drug': 5  # Similar to alcohol and tobacco\n",
        "}\n",
        "\n",
        "# Set base path to current working directory in Colab\n",
        "project_base_path = Path(\"/content\").resolve()\n",
        "\n",
        "# Example function call, adjust according to your needs\n",
        "model = Model(config['sdoh'], sdoh_label_counts[config['sdoh']], config['model'], config['epochs'], config['batch'], project_base_path, config['balanced'], config['weighted'], output_dir=config['output'], cv=config['cv'])\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "JXJCKUwtjIRf",
        "outputId": "4447698d-689a-4361-ce52-f3d26b7d5e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'train_dataset' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5671ec7cd709>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdoh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdoh_label_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdoh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-2381ec25bdab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m               \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m               \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m               \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m               \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_dataset' referenced before assignment"
          ]
        }
      ]
    }
  ]
}